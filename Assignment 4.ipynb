{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<a href=\"https://colab.research.google.com/github/shafayat1004/Data-Mining-Assignments/blob/main/Assignment%204.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# KMeans Clustering on MNIST Dataset\n",
    "## 1910456 Mir Shafayat Ahmed                                                                                                     "
   ],
   "metadata": {
    "id": "CsvLrbDMV97W"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import struct\n",
    "from array import array\n",
    "from os.path  import join\n",
    "import plotly.express as px\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "!mkdir -p mnist/t10k-images-idx3-ubyte\n",
    "!mkdir -p mnist/t10k-labels-idx1-ubyte\n",
    "!mkdir -p mnist/train-images-idx3-ubyte\n",
    "!mkdir -p mnist/train-labels-idx1-ubyte\n",
    "\n",
    "!curl \"https://raw.githubusercontent.com/shafayat1004/Data-Mining-Assignments/main/mnist/t10k-images-idx3-ubyte/t10k-images-idx3-ubyte\" --output mnist/t10k-images-idx3-ubyte/t10k-images-idx3-ubyte\n",
    "\n",
    "!curl \"https://raw.githubusercontent.com/shafayat1004/Data-Mining-Assignments/main/mnist/t10k-labels-idx1-ubyte/t10k-labels-idx1-ubyte\" --output mnist/t10k-labels-idx1-ubyte/t10k-labels-idx1-ubyte\n",
    "\n",
    "!curl \"https://raw.githubusercontent.com/shafayat1004/Data-Mining-Assignments/main/mnist/train-images-idx3-ubyte/train-images-idx3-ubyte\" --output mnist/train-images-idx3-ubyte/train-images-idx3-ubyte\n",
    "\n",
    "!curl \"https://raw.githubusercontent.com/shafayat1004/Data-Mining-Assignments/main/mnist/train-labels-idx1-ubyte/train-labels-idx1-ubyte\" --output mnist/train-labels-idx1-ubyte/train-labels-idx1-ubyte\n",
    "\n",
    "divider = \"--------------------------------------------------------------------------------------\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-04T14:23:34.102489200Z",
     "start_time": "2023-09-04T14:23:34.055131100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#\n",
    "# MNIST Data Loader Class\n",
    "#\n",
    "class MnistDataloader(object):\n",
    "    def __init__(\n",
    "            self,\n",
    "            training_images_filepath,\n",
    "            training_labels_filepath,\n",
    "            test_images_filepath,\n",
    "            test_labels_filepath\n",
    "    ):\n",
    "        self.training_images_filepath = training_images_filepath\n",
    "        self.training_labels_filepath = training_labels_filepath\n",
    "        self.test_images_filepath     = test_images_filepath\n",
    "        self.test_labels_filepath     = test_labels_filepath\n",
    "\n",
    "    def read_images_labels(self, images_filepath, labels_filepath):\n",
    "        labels = []\n",
    "\n",
    "        with open(labels_filepath, 'rb') as file:\n",
    "            magic, size = struct.unpack(\">II\", file.read(8))\n",
    "\n",
    "            if magic != 2049:\n",
    "                raise ValueError('Magic number mismatch, expected 2049, got {}'.format(magic))\n",
    "\n",
    "            labels = array(\"B\", file.read())\n",
    "\n",
    "        with open(images_filepath, 'rb') as file:\n",
    "            magic, size, rows, cols = struct.unpack(\">IIII\", file.read(16))\n",
    "\n",
    "            if magic != 2051:\n",
    "                raise ValueError('Magic number mismatch, expected 2051, got {}'.format(magic))\n",
    "\n",
    "            image_data = array(\"B\", file.read())\n",
    "\n",
    "        images = []\n",
    "\n",
    "        for i in range(size):\n",
    "            images.append([0] * rows * cols)\n",
    "\n",
    "        for i in range(size):\n",
    "            img          = np.array(image_data[i * rows * cols:(i + 1) * rows * cols])\n",
    "            images[i][:] = img\n",
    "\n",
    "        return images, labels\n",
    "\n",
    "    def load_data(self):\n",
    "        train_images, train_labels = self.read_images_labels(self.training_images_filepath, self.training_labels_filepath)\n",
    "        test_images,  test_labels  = self.read_images_labels(self.test_images_filepath,     self.test_labels_filepath)\n",
    "        return (train_images, train_labels), (test_images, test_labels)"
   ],
   "metadata": {
    "id": "RkLX9kQDWMLy",
    "ExecuteTime": {
     "end_time": "2023-09-04T14:23:34.118166100Z",
     "start_time": "2023-09-04T14:23:34.070871400Z"
    }
   },
   "execution_count": 120,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "simple centroid initialization function"
   ],
   "metadata": {
    "id": "RvgJK_f_WOlP"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def initialize_centroids_simple(data, dimension_of_data, no_of_clusters):\n",
    "    #centroids: [[centroid0:  3 dimensions, , , ]; [centroid1: 3 dimensions ] ... ..]\n",
    "    centroids = np.zeros((no_of_clusters, dimension_of_data))\n",
    "    \n",
    "    random_indexes = np.random.permutation(len(data))                                                # Selecting random data points for each initial Centroid\n",
    "    for i in range(no_of_clusters):\n",
    "        centroids[i] = data[random_indexes[i]]\n",
    "        \n",
    "    print(f\"Initialized Random Centroids =\\n{centroids}\")\n",
    "    print(divider)\n",
    "    \n",
    "    return centroids"
   ],
   "metadata": {
    "id": "xZm_BPcEWTlh",
    "ExecuteTime": {
     "end_time": "2023-09-04T14:23:34.118166100Z",
     "start_time": "2023-09-04T14:23:34.086413500Z"
    }
   },
   "execution_count": 121,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Centroid initilization using min max"
   ],
   "metadata": {
    "id": "ZLyNeHzdWnTo"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Calculate eucledian distance"
   ],
   "metadata": {
    "id": "J3vAsctEWwYP"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def get_euclidean_distance(vector1, vector2):\n",
    "    difference = vector1 - vector2\n",
    "    sum_of_squared_diff = np.dot (difference, difference)\n",
    "    distance = np.sqrt(sum_of_squared_diff)\n",
    "    \n",
    "    return distance"
   ],
   "metadata": {
    "id": "VfZobkk-W2FA",
    "ExecuteTime": {
     "end_time": "2023-09-04T14:23:34.118166100Z",
     "start_time": "2023-09-04T14:23:34.102489200Z"
    }
   },
   "execution_count": 122,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "outputs": [],
   "source": [
    "def clustering_objective (data, centroids):\n",
    "    total_sum = float(0)                                                                             # Initial total_sum is 0\n",
    "    for point in data:                                                                               # Iterating through each datapoint\n",
    "        total_sum += min([np.dot(point - centroid, point - centroid) for centroid in centroids])     # List comprehension to get a list of all the squared distances between...\n",
    "                                                                                                     # ...the current datapoint and each of the k centroids. Then using min()...\n",
    "                                                                                                     # ...to get the minimum distance and adding it to sum.\n",
    "    return total_sum/float(len(data))                                                                # J is found by dividing sum with the number of datapoints. 1/N * sum"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-04T14:23:34.133773800Z",
     "start_time": "2023-09-04T14:23:34.118166100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "KMeans Function\n"
   ],
   "metadata": {
    "id": "YvA2jiqiXBvv"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def kmeans(data, dimension_of_data, no_of_clusters, max_iterations):\n",
    "    no_of_features = np.size(data, 0)\n",
    "    \n",
    "    #centroids: [[centroid0:  , , ,3 dimensions, , , ],  [centroid1: , , ,3 dimensions, , , ],  ... ..]\n",
    "    centroids = initialize_centroids_simple(data, dimension_of_data, no_of_clusters)\n",
    "    #cluster_affiliation: cluster_affiliation = [clusternumber, clutsernumber, ..., ..., ..., ...]\n",
    "    \n",
    "    #initialize the cluster affiliations. Initially assign -1\n",
    "    cluster_affiliation = (np.zeros(len(data)) - 1).astype(int)\n",
    "    print(f\"Initial Cluster Affiliation =\\n{cluster_affiliation}\")\n",
    "    print(divider)\n",
    "    \n",
    "    continue_loop = True\n",
    "    j_prev = np.inf                                                                                  # So that loop does not break immediately after first Iteration.\n",
    "    for iteration in range(max_iterations):                                                          # Setting a max limit of loops so that it is not infinite.\n",
    "        print(f\"Iteration {iteration}:\")\n",
    "        for i, point in enumerate(data): #use numpy equivalent code\n",
    "            \n",
    "            min_distance = float('inf')\n",
    "            min_distance_index = None\n",
    "\n",
    "            #find the closest centroids for each data points\n",
    "            for cluster_index, centroid in enumerate(centroids): #use numpy equivalent code\n",
    "                distance = get_euclidean_distance(centroid, point)\n",
    "                if distance < min_distance:\n",
    "                    min_distance = distance\n",
    "                    min_distance_index = cluster_index\n",
    "\n",
    "            #record or update cluster for each data points\n",
    "            if cluster_affiliation[i] != min_distance_index:\n",
    "               cluster_affiliation[i] = min_distance_index\n",
    "                \n",
    "        #recompute centroids\n",
    "        for cluster_index in range(no_of_clusters):                                                  # Iterating over all centroid indexes\n",
    "            grouped_points = data[np.take(cluster_affiliation, cluster_index)]                              # The actual datapoints closest to the current centroid are extracted by using a boolean array \n",
    "            one_vector = np.ones(len(grouped_points))\n",
    "            centroids[cluster_index] = (np.dot(one_vector, grouped_points) / len(grouped_points))    # The new centroid is now the mean of the grouped datapoints\n",
    "\n",
    "        j = clustering_objective (data, centroids)\n",
    "    \n",
    "        print(f\"New centroids in this iteration =\\n{centroids}\")\n",
    "        print(f\"|J - J_prev| = {abs(j - j_prev)}\")\n",
    "        print(divider)\n",
    "        \n",
    "        if abs(j - j_prev) <= 10**(-5):                                                              # Breaks the loop if there is no significant change in the value of J over an iteration\n",
    "            break\n",
    "        j_prev = j                                                                                   # Updating J_prev for later iterations.\n",
    "\n",
    "    print(f\"Final Centroids =\\n{centroids}\")\n",
    "    print(f\"Final Cluster Affiliations =\\n{cluster_affiliation}\")\n",
    "    return centroids, cluster_affiliation\n"
   ],
   "metadata": {
    "id": "G-5U2bt-XI8g",
    "ExecuteTime": {
     "end_time": "2023-09-04T14:23:34.149388800Z",
     "start_time": "2023-09-04T14:23:34.133773800Z"
    }
   },
   "execution_count": 124,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Driver funtion/Main Function"
   ],
   "metadata": {
    "id": "XduHk-N0XKQO"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "outputs": [],
   "source": [
    "def create_3d_scatter_plot_of_dataset (data, class_labels):\n",
    "    flower_names = class_labels.unique()\n",
    "    symbols_dict = {\n",
    "        flower_names[0] : \"square-open\",\n",
    "        flower_names[1] : \"diamond-open\",\n",
    "        flower_names[2] : \"circle-open\"\n",
    "    }\n",
    "\n",
    "    dataset_3d_plot = \\\n",
    "        px.scatter_3d (\n",
    "            title      = \"Dataset\",\n",
    "            data_frame = data,\n",
    "            x          = data.columns[0],\n",
    "            y          = data.columns[1],\n",
    "            z          = data.columns[2],\n",
    "            color      = class_labels,\n",
    "            height     = 600\n",
    "        )\n",
    "    \n",
    "    for i, d in enumerate(dataset_3d_plot.data):\n",
    "        dataset_3d_plot.data[i].marker.symbol = symbols_dict[dataset_3d_plot.data[i].name]\n",
    "    \n",
    "    return dataset_3d_plot"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-04T14:23:34.165040500Z",
     "start_time": "2023-09-04T14:23:34.149388800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "outputs": [],
   "source": [
    "def create_3d_scatter_plot_showing_cluster_affiliation (data, cluster_affiliation):\n",
    "    clusters = np.unique(cluster_affiliation)\n",
    "    symbols_dict = {\n",
    "        clusters[0] : \"square-open\",\n",
    "        clusters[1] : \"diamond-open\",\n",
    "        clusters[2] : \"circle-open\"\n",
    "    }\n",
    "    \n",
    "    data[\"Cluster Affiliation\"] = cluster_affiliation\n",
    "\n",
    "    dataset_3d_plot = \\\n",
    "        px.scatter_3d (\n",
    "            title      = \"Cluster Affiliation\",\n",
    "            data_frame = data,\n",
    "            x          = data.columns[0],\n",
    "            y          = data.columns[1],\n",
    "            z          = data.columns[2],\n",
    "            color      = data.columns[3],\n",
    "            height     = 600\n",
    "        )\n",
    "    \n",
    "    for i, d in enumerate(dataset_3d_plot.data):\n",
    "        dataset_3d_plot.data[i].marker.symbol = symbols_dict[dataset_3d_plot.data[i].name]\n",
    "        \n",
    "    return dataset_3d_plot"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-04T14:23:34.180665900Z",
     "start_time": "2023-09-04T14:23:34.165040500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "outputs": [],
   "source": [
    "def get_most_frequent (arr):\n",
    "    return np.argmax(np.bincount(arr))\n",
    "\n",
    "def create_3d_scatter_plot_showing_wrong_affiliation (data, cluster_affiliation, class_labels):\n",
    "    data[\"Cluster Affiliation\"] = cluster_affiliation\n",
    "    data[\"Actual Class\"] = class_labels\n",
    "    most_common_affiliations = [get_most_frequent(cluster_affiliation[start : end]) for start, end in [(0, 50), (50, 100), (100, 150)]]\n",
    "    \n",
    "    affiliation_to_flower_name_dict ={\n",
    "        most_common_affiliations[0]: class_labels[0],\n",
    "        most_common_affiliations[1]: class_labels[50],\n",
    "        most_common_affiliations[2]: class_labels[100]\n",
    "    }\n",
    "\n",
    "    data[\"Cluster Affiliation\"] = data[\"Cluster Affiliation\"].map(affiliation_to_flower_name_dict)\n",
    "    data[\"Is Correct\"] = data[\"Cluster Affiliation\"] == data[\"Actual Class\"]\n",
    "    \n",
    "    print(\"Data:\")\n",
    "    display(data)\n",
    "    \n",
    "    cluster_correctness_3d_scatter_plot = \\\n",
    "        px.scatter_3d (\n",
    "            title      = \"Cluster Correctness\",\n",
    "            data_frame = data,\n",
    "            x          = data.columns[0],\n",
    "            y          = data.columns[1],\n",
    "            z          = data.columns[2],\n",
    "            color      = \"Is Correct\",\n",
    "            height     = 600\n",
    "        )\n",
    "    \n",
    "    if cluster_correctness_3d_scatter_plot.data[0].name == \"True\":\n",
    "        cluster_correctness_3d_scatter_plot.data[0].marker.color = \"green\"\n",
    "    else:\n",
    "        cluster_correctness_3d_scatter_plot.data[0].marker.color = \"red\"\n",
    "        \n",
    "    if cluster_correctness_3d_scatter_plot.data[1].name == \"True\":\n",
    "        cluster_correctness_3d_scatter_plot.data[1].marker.color = \"green\"\n",
    "    else:\n",
    "        cluster_correctness_3d_scatter_plot.data[1].marker.color = \"red\"\n",
    "\n",
    "    return cluster_correctness_3d_scatter_plot\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-04T14:23:34.199314700Z",
     "start_time": "2023-09-04T14:23:34.180665900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def main():\n",
    "    no_of_clusters = 10                                                   # K clusters\n",
    "    no_of_training_data = 10000\n",
    "    \n",
    "    print(f\"Number of clusters = {no_of_clusters}\")\n",
    "\n",
    "    # Setting file paths based on added MNIST Datasets\n",
    "    input_path = 'mnist/'\n",
    "    training_images_filepath = join(input_path, 'train-images-idx3-ubyte/train-images-idx3-ubyte')\n",
    "    training_labels_filepath = join(input_path, 'train-labels-idx1-ubyte/train-labels-idx1-ubyte')\n",
    "    test_images_filepath     = join(input_path, 't10k-images-idx3-ubyte/t10k-images-idx3-ubyte')\n",
    "    test_labels_filepath     = join(input_path, 't10k-labels-idx1-ubyte/t10k-labels-idx1-ubyte')\n",
    "    \n",
    "    # Loading MINST dataset\n",
    "    mnist_dataloader = MnistDataloader(training_images_filepath, training_labels_filepath, test_images_filepath, test_labels_filepath)\n",
    "    \n",
    "    (train_images, train_labels), (test_images, test_labels) = mnist_dataloader.load_data()\n",
    "    \n",
    "    train_images, train_labels = train_images[:no_of_training_data], train_labels[:no_of_training_data]\n",
    "    \n",
    "    \n",
    "    print(f\"Length of train set = {len(train_images)}\")\n",
    "    print(f\"Length of test set = {len(test_images)}\")\n",
    "    \n",
    "    dimension_of_data = np.size(train_images, 1)                                                             # number of  data dimension in the data\n",
    "    \n",
    "    print(f\"Dimensions being used = {dimension_of_data}\")\n",
    "    print(divider)\n",
    "    \n",
    "    centroids, cluster_affiliation = kmeans(train_images, dimension_of_data, no_of_clusters, 300)\n",
    "    \n",
    "    display(centroids)\n",
    "\t\n",
    "if __name__ == \"__main__\":\n",
    "\tmain()\n",
    "\t\t\t"
   ],
   "metadata": {
    "id": "kvq2KvqcXQOf",
    "ExecuteTime": {
     "end_time": "2023-09-04T14:24:08.568370Z",
     "start_time": "2023-09-04T14:23:34.199314700Z"
    }
   },
   "execution_count": 128,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters = 10\n",
      "Length of train set = 20000\n",
      "Length of test set = 10000\n",
      "Dimensions being used = 784\n",
      "--------------------------------------------------------------------------------------\n",
      "Initialized Random Centroids =\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "--------------------------------------------------------------------------------------\n",
      "Initial Cluster Affiliation =\n",
      "[-1 -1 -1 ... -1 -1 -1]\n",
      "--------------------------------------------------------------------------------------\n",
      "Iteration 0:\n",
      "New centroids in this iteration =\n",
      "[[13.86989796 13.86989796 13.86989796 ... 13.86989796 13.86989796\n",
      "  13.86989796]\n",
      " [13.86989796 13.86989796 13.86989796 ... 13.86989796 13.86989796\n",
      "  13.86989796]\n",
      " [21.85586735 21.85586735 21.85586735 ... 21.85586735 21.85586735\n",
      "  21.85586735]\n",
      " ...\n",
      " [13.86989796 13.86989796 13.86989796 ... 13.86989796 13.86989796\n",
      "  13.86989796]\n",
      " [37.75637755 37.75637755 37.75637755 ... 37.75637755 37.75637755\n",
      "  37.75637755]\n",
      " [21.85586735 21.85586735 21.85586735 ... 21.85586735 21.85586735\n",
      "  21.85586735]]\n",
      "|J - J_prev| = inf\n",
      "--------------------------------------------------------------------------------------\n",
      "Iteration 1:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[128], line 35\u001B[0m\n\u001B[0;32m     32\u001B[0m     display(centroids)\n\u001B[0;32m     34\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m---> 35\u001B[0m \t\u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[128], line 30\u001B[0m, in \u001B[0;36mmain\u001B[1;34m()\u001B[0m\n\u001B[0;32m     27\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDimensions being used = \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdimension_of_data\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     28\u001B[0m \u001B[38;5;28mprint\u001B[39m(divider)\n\u001B[1;32m---> 30\u001B[0m centroids, cluster_affiliation \u001B[38;5;241m=\u001B[39m \u001B[43mkmeans\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_images\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdimension_of_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mno_of_clusters\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m300\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     32\u001B[0m display(centroids)\n",
      "Cell \u001B[1;32mIn[124], line 39\u001B[0m, in \u001B[0;36mkmeans\u001B[1;34m(data, dimension_of_data, no_of_clusters, max_iterations)\u001B[0m\n\u001B[0;32m     36\u001B[0m     one_vector \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mones(\u001B[38;5;28mlen\u001B[39m(grouped_points))\n\u001B[0;32m     37\u001B[0m     centroids[cluster_index] \u001B[38;5;241m=\u001B[39m (np\u001B[38;5;241m.\u001B[39mdot(one_vector, grouped_points) \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mlen\u001B[39m(grouped_points))    \u001B[38;5;66;03m# The new centroid is now the mean of the grouped datapoints\u001B[39;00m\n\u001B[1;32m---> 39\u001B[0m j \u001B[38;5;241m=\u001B[39m \u001B[43mclustering_objective\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcentroids\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     41\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNew centroids in this iteration =\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mcentroids\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     42\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m|J - J_prev| = \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mabs\u001B[39m(j\u001B[38;5;250m \u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;250m \u001B[39mj_prev)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[1;32mIn[123], line 4\u001B[0m, in \u001B[0;36mclustering_objective\u001B[1;34m(data, centroids)\u001B[0m\n\u001B[0;32m      2\u001B[0m total_sum \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mfloat\u001B[39m(\u001B[38;5;241m0\u001B[39m)                                                                             \u001B[38;5;66;03m# Initial total_sum is 0\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m point \u001B[38;5;129;01min\u001B[39;00m data:                                                                               \u001B[38;5;66;03m# Iterating through each datapoint\u001B[39;00m\n\u001B[1;32m----> 4\u001B[0m     total_sum \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmin\u001B[39m([np\u001B[38;5;241m.\u001B[39mdot(point \u001B[38;5;241m-\u001B[39m centroid, point \u001B[38;5;241m-\u001B[39m centroid) \u001B[38;5;28;01mfor\u001B[39;00m centroid \u001B[38;5;129;01min\u001B[39;00m centroids])     \u001B[38;5;66;03m# List comprehension to get a list of all the squared distances between...\u001B[39;00m\n\u001B[0;32m      5\u001B[0m                                                                                                  \u001B[38;5;66;03m# ...the current datapoint and each of the k centroids. Then using min()...\u001B[39;00m\n\u001B[0;32m      6\u001B[0m                                                                                                  \u001B[38;5;66;03m# ...to get the minimum distance and adding it to sum.\u001B[39;00m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m total_sum\u001B[38;5;241m/\u001B[39m\u001B[38;5;28mfloat\u001B[39m(\u001B[38;5;28mlen\u001B[39m(data))\n",
      "Cell \u001B[1;32mIn[123], line 4\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m      2\u001B[0m total_sum \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mfloat\u001B[39m(\u001B[38;5;241m0\u001B[39m)                                                                             \u001B[38;5;66;03m# Initial total_sum is 0\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m point \u001B[38;5;129;01min\u001B[39;00m data:                                                                               \u001B[38;5;66;03m# Iterating through each datapoint\u001B[39;00m\n\u001B[1;32m----> 4\u001B[0m     total_sum \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmin\u001B[39m([np\u001B[38;5;241m.\u001B[39mdot(point \u001B[38;5;241m-\u001B[39m centroid, \u001B[43mpoint\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mcentroid\u001B[49m) \u001B[38;5;28;01mfor\u001B[39;00m centroid \u001B[38;5;129;01min\u001B[39;00m centroids])     \u001B[38;5;66;03m# List comprehension to get a list of all the squared distances between...\u001B[39;00m\n\u001B[0;32m      5\u001B[0m                                                                                                  \u001B[38;5;66;03m# ...the current datapoint and each of the k centroids. Then using min()...\u001B[39;00m\n\u001B[0;32m      6\u001B[0m                                                                                                  \u001B[38;5;66;03m# ...to get the minimum distance and adding it to sum.\u001B[39;00m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m total_sum\u001B[38;5;241m/\u001B[39m\u001B[38;5;28mfloat\u001B[39m(\u001B[38;5;28mlen\u001B[39m(data))\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ]
  }
 ]
}
